{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMD3zQuvh4bUkznr7y64r6A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["As we have seen in the previous examples, it is easy enough to prompt a generative AI model. Shoot off an API call, and suddently you have an answer, a machine translation, sentiment analyzed, or a chat message generated. However, going from \"prompting\" to **prompt engineering** and the engineering of your AI model based processes is a bit more involved. The importance of the \"engineering\" in prompt engineering has become increasingly apparent, as models have become more complex and powerful, and the demand for more accurate and interpretable results has grown.\n","\n","The ability to engineer effective prompts allows us to configure and tune model responses to better suit our specific needs (e.g., for a particular industry like healthcare), whether we are trying to improve the quality of the output, reduce bias, or optimize for efficiency. By leveraging the principles of prompt engineering, we can ensure that our models are not only accurate and reliable, but also robust and scalable."],"metadata":{"id":"RCsog1OfZjeC"}},{"cell_type":"markdown","source":["# Dependencies and imports"],"metadata":{"id":"WUhCP56_Zm7S"}},{"cell_type":"code","execution_count":31,"metadata":{"id":"vgp13t_g6SPk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683717773001,"user_tz":240,"elapsed":11356,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"1f2e9ac1-ae00-4272-d98b-af1096034df2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: predictionguard in /usr/local/lib/python3.10/dist-packages (0.2.4)\n","Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.163)\n","Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n","Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.7)\n","Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n","Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.2.4)\n","Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n","Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.65.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n","Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n","Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"]}],"source":["! pip install predictionguard langchain"]},{"cell_type":"code","source":["import predictionguard as pg\n","from langchain import PromptTemplate\n","from langchain import PromptTemplate, FewShotPromptTemplate\n","import numpy as np"],"metadata":{"id":"FbbtCowOPNEM","executionInfo":{"status":"ok","timestamp":1683717828970,"user_tz":240,"elapsed":7,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["client = pg.Client(token=\"n4HehSxYpKxQyhKX58IzqPjXa2pOOJ\")"],"metadata":{"id":"uekOso_tPY8h","executionInfo":{"status":"ok","timestamp":1683717829486,"user_tz":240,"elapsed":285,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["# Prompt Templates"],"metadata":{"id":"nQa7oxnrQJaG"}},{"cell_type":"markdown","source":["One of the best practices that we will discuss below involves testing and evaluating model output using example prompt contexts and formulations. In order to institute this practice, we need a way to rapidly and programmatically format prompts with a variety of contexts. We will need this in our applications anyway, because in production we will be receiving dynamic input from the user or another application. That dynamic input (or something extracted from it) will be inserted into our prompts on-the-fly. We already saw in the last notebook a prompt that included a bunch of boilerplate:"],"metadata":{"id":"wx_4V15vZ3jx"}},{"cell_type":"markdown","source":["## Zero shot Q&A"],"metadata":{"id":"ln87IJ2MQW7I"}},{"cell_type":"code","source":["template = \"\"\"Read the context below and answer the question. If the question cannot be answered based on the context alone or the context does not explicitly say the answer to the question, write \"Sorry I had trouble answering this question, based on the information I found.\"\n","\n","Context: {context}\n","\n","Question: {question}\n","\n","Answer: \"\"\"\n"," \n","prompt = PromptTemplate(\n","    input_variables=[\"context\", \"question\"],\n","    template=template,\n",")"],"metadata":{"id":"uDCv4-2vPnai"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["context = \"Domino's gift cards are great for any person and any occasion. There are a number of different options to choose from. Each comes with a personalized card carrier and is delivered via US Mail.\"\n","\n","question = \"How are gift cards delivered?\"\n","\n","myprompt = prompt.format(context=context, question=question)\n","print(myprompt)"],"metadata":{"id":"zR4a7J-vQOvx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683705727815,"user_tz":240,"elapsed":264,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"3249b779-12ca-4a94-cb61-31e0034c74a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Read the context below and answer the question. If the question cannot be answered based on the context alone or the context does not explicitly say the answer to the question, write \"Sorry I had trouble answering this question, based on the information I found.\"\n","\n","Context: Domino's gift cards are great for any person and any occasion. There are a number of different options to choose from. Each comes with a personalized card carrier and is delivered via US Mail.\n","\n","Question: How are gift cards delivered?\n","\n","Answer: \n"]}]},{"cell_type":"markdown","source":["## Few Shot - Sentiment"],"metadata":{"id":"icmPu-1wQYsS"}},{"cell_type":"markdown","source":["This kind of prompt template could in theory be flexible to create zero shot or few shot prompts. However, LangChain provides a bit more convenience for few shot prompts. We can first create a template for individual demonstrations within the few shot prompt:"],"metadata":{"id":"pkxo3WElaEFy"}},{"cell_type":"code","source":["# Create a string formatter for sentiment analysis demonstrations.\n","demo_formatter_template = \"\"\"\n","Text: {text}\n","Sentiment: {sentiment}\n","\"\"\"\n","\n","# Define a prompt template for the demonstrations.\n","demo_prompt = PromptTemplate(\n","    input_variables=[\"text\", \"sentiment\"],\n","    template=demo_formatter_template,\n",")"],"metadata":{"id":"OFzSkr9iQREn","executionInfo":{"status":"ok","timestamp":1683717906910,"user_tz":240,"elapsed":224,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# Each row here includes:\n","# 1. an example text input (that we want to analyze for sentiment)\n","# 2. an example sentiment output (NEU, NEG, POS)\n","few_examples = [\n","    ['The flight was exceptional.', 'POS'],\n","    ['That pilot is adorable.', 'POS'],\n","    ['This was an awful seat.', 'NEG'],\n","    ['This pilot was brilliant.', 'POS'],\n","    ['I saw the aircraft.', 'NEU'],\n","    ['That food was exceptional.', 'POS'],\n","    ['That was a private aircraft.', 'NEU'],\n","    ['This is an unhappy pilot.', 'NEG'],\n","    ['The staff is rough.', 'NEG'],\n","    ['This staff is Australian.', 'NEU']\n","]\n","examples = []\n","for ex in few_examples:\n","  examples.append({\n","      \"text\": ex[0],\n","      \"sentiment\": ex[1]\n","  })"],"metadata":{"id":"FFIr_kHSQez3","executionInfo":{"status":"ok","timestamp":1683717907506,"user_tz":240,"elapsed":354,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["few_shot_prompt = FewShotPromptTemplate(\n","    \n","    # This is the demonstration data we want to insert into the prompt.\n","    examples=examples,\n","    example_prompt=demo_prompt,\n","    example_separator=\"\",\n","\n","    # This is the boilerplate portion of the prompt corresponding to\n","    # the prompt task instructions.\n","    prefix=\"Classify the sentiment of the text. Use the label NEU for neutral sentiment, NEG for negative sentiment, and POS for positive sentiment.\\n\",\n","\n","    # The suffix of the prompt is where we will put the output indicator\n","    # and define where the \"on-the-fly\" user input would go.\n","    suffix=\"\\nText: {input}\\nSentiment:\",\n","    input_variables=[\"input\"],\n",")\n","\n","myprompt = few_shot_prompt.format(input=\"The flight is boring.\")\n","print(myprompt)"],"metadata":{"id":"Edbb1OogQinc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683717908130,"user_tz":240,"elapsed":9,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"9b53ab48-cfe0-48b0-f2c6-c83c0c8b0e26"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Classify the sentiment of the text. Use the label NEU for neutral sentiment, NEG for negative sentiment, and POS for positive sentiment.\n","\n","Text: The flight was exceptional.\n","Sentiment: POS\n","\n","Text: That pilot is adorable.\n","Sentiment: POS\n","\n","Text: This was an awful seat.\n","Sentiment: NEG\n","\n","Text: This pilot was brilliant.\n","Sentiment: POS\n","\n","Text: I saw the aircraft.\n","Sentiment: NEU\n","\n","Text: That food was exceptional.\n","Sentiment: POS\n","\n","Text: That was a private aircraft.\n","Sentiment: NEU\n","\n","Text: This is an unhappy pilot.\n","Sentiment: NEG\n","\n","Text: The staff is rough.\n","Sentiment: NEG\n","\n","Text: This staff is Australian.\n","Sentiment: NEU\n","\n","Text: The flight is boring.\n","Sentiment:\n"]}]},{"cell_type":"markdown","source":["## Few Shot - Text Classification"],"metadata":{"id":"_f8H6HdUQzG-"}},{"cell_type":"code","source":["demo_formatter_template = \"\"\"\\nText: {text}\n","Categories: {categories}\n","Class: {class}\\n\"\"\"\n","\n","# Define a prompt template for the demonstrations.\n","demo_prompt = PromptTemplate(\n","    input_variables=[\"text\", \"categories\", \"class\"],\n","    template=demo_formatter_template,\n",")\n","\n","# Each row here includes:\n","# 1. an example set of categories for the text classification\n","# 2. an example text that we want to classify\n","# 3. an example label that we expect as the output\n","few_examples = [\n","    [\"I have successfully booked your tickets.\", \"agent, customer\", \"agent\"],\n","    [\"What's the oldest building in US?\", \"quantity, location\", \"location\"],\n","    [\"This video game is amazing. I love it!\", \"positive, negative\", \"\"],\n","    [\"Dune is the best movie ever.\", \"cinema, art, music\", \"cinema\"]\n","]\n","examples = []\n","for ex in few_examples:\n","  examples.append({\n","      \"text\": ex[0],\n","      \"categories\": ex[1],\n","      \"class\": ex[2]\n","  })\n","\n","few_shot_prompt = FewShotPromptTemplate(\n","    \n","    # This is the demonstration data we want to insert into the prompt.\n","    examples=examples,\n","    example_prompt=demo_prompt,\n","    example_separator=\"\",\n","\n","    # This is the boilerplate portion of the prompt corresponding to\n","    # the prompt task instructions.\n","    prefix=\"Classify the following input text into one of the given categories.\\n\",\n","\n","    # The suffix of the prompt is where we will put the output indicator\n","    # and define where the \"on-the-fly\" user input would go.\n","    suffix=\"\\nText: {text}\\nCategories: {categories}\\nClass: \",\n","    input_variables=[\"text\", \"categories\"],\n",")\n","\n","myprompt = few_shot_prompt.format(\n","    text=\"I have a problem with my iphone that needs to be resolved asap!\",\n","    categories=\"urgent, not urgent\")\n","print(myprompt)"],"metadata":{"id":"0m_Xo7F4QmUA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683717911696,"user_tz":240,"elapsed":241,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"e8a8c251-d97c-406d-dea6-697c92845b5d"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Classify the following input text into one of the given categories.\n","\n","Text: I have successfully booked your tickets.\n","Categories: agent, customer\n","Class: agent\n","\n","Text: What's the oldest building in US?\n","Categories: quantity, location\n","Class: location\n","\n","Text: This video game is amazing. I love it!\n","Categories: positive, negative\n","Class: \n","\n","Text: Dune is the best movie ever.\n","Categories: cinema, art, music\n","Class: cinema\n","\n","Text: I have a problem with my iphone that needs to be resolved asap!\n","Categories: urgent, not urgent\n","Class: \n"]}]},{"cell_type":"code","source":["client.predict(name=\"default-text-gen\", data={\n","    \"prompt\": myprompt\n","})"],"metadata":{"id":"SawFqRg6Q25L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683717917174,"user_tz":240,"elapsed":3857,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"78f990a2-2a70-4036-b443-ec4a12990222"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'text': '\\nUrgent'}"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["# Parameters"],"metadata":{"id":"LnqGMvSlS8jc"}},{"cell_type":"markdown","source":["Although we have most sent a single text prompt the models to get a response. There is configurability via parameters such as `temperature` and `max_tokens`. Optimizing model parameters can help us achieve a desired output."],"metadata":{"id":"CTGEyW24aYQS"}},{"cell_type":"markdown","source":["## Temperature"],"metadata":{"id":"UKdT9L62O0HL"}},{"cell_type":"code","source":["for temp in np.arange(0.0, 1.0, 0.4):\n","  print(\"\\nTemperature: \", temp)\n","  print(\"----------------------------\")\n","  for i in range(0,3):\n","    completion = client.predict(name=\"default-text-gen\", data={\n","        \"prompt\": \"A great name for a unknown wizard (other than Gandalf and Radagast) from the Lord of the Rings universe is:\",\n","        \"temperature\": temp\n","    })['text'].strip()\n","    print(completion)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jk1CssKiPG2v","executionInfo":{"status":"ok","timestamp":1683665389517,"user_tz":240,"elapsed":19723,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"11c53251-4631-42c1-cce4-e31a8e228b63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Temperature:  0.0\n","----------------------------\n","Mithrandir.\n","Mithrandir.\n","Merlinus the Magnificent.\n","\n","Temperature:  0.4\n","----------------------------\n","Merlinus the Magnificent.\n","Merlindor.\n","Merlinor the Magnificent.\n","\n","Temperature:  0.8\n","----------------------------\n","Merlinus the White.\n","Vardaol.\n","Alatar the Blue.\n"]}]},{"cell_type":"markdown","source":["## Max Tokens"],"metadata":{"id":"Yi9opA-WO2H2"}},{"cell_type":"code","source":["for tokens in range(20, 200, 80):\n","  print(\"\\nMax Tokens: \", tokens)\n","  print(\"----------------------------\")\n","  completion = client.predict(name=\"default-text-gen\", data={\n","      \"prompt\": \"Write a story about Alatar the Blue, a great wizard from the Lord of the Rings.\",\n","      \"temperature\": 0.8,\n","      \"max_tokens\": tokens\n","  })['text'].strip()\n","  print(completion)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sFD32z3JO5Mz","executionInfo":{"status":"ok","timestamp":1683704892473,"user_tz":240,"elapsed":39547,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"4a18b18d-cd8f-453f-a2de-5cb06b9c87e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Max Tokens:  20\n","----------------------------\n","Once upon a time, in a distant and forgotten land, there lived a great wizard,\n","\n","Max Tokens:  100\n","----------------------------\n","It was said that Alatar the Blue was one of the greatest wizards of Middle-earth, and his powers were almost unmatched. He was one of the five Maiar that had been sent to Middle-earth to help the Free Peoples in their struggle against the forces of the Dark Lord Sauron.\n","\n","Alatar was known for his wisdom and knowledge, and his magical powers were almost unparalleled. He was greatly respected by all who knew him, and his counsel was sought by kings and\n","\n","Max Tokens:  180\n","----------------------------\n","Once upon a time, in the faraway land of Middle-Earth, there lived a great and powerful wizard by the name of Alatar the Blue. He was a mysterious figure, shrouded in mystery and legend.\n","\n","Alatar was known to be one of the wisest and most powerful of the Istari, a group of five wizards sent by the Valar, god-like beings, to Middle-Earth to help in the fight against the forces of evil. He was wise and powerful, but also kind and gentle, and many people across Middle-Earth trusted and respected him.\n","\n","Alatar was said to be the most powerful of the Istari, and his mastery of spells and his skill with the staff was unparalleled. He could create powerful enchantments and cast powerful spells, and he was even known to have the ability to summon spirits from the dead\n"]}]},{"cell_type":"markdown","source":["# Multiple formulations"],"metadata":{"id":"luY4EKZhTbEB"}},{"cell_type":"markdown","source":["Why settle for a single prompt and/or set of parameters when you can use mutliple. Try using multiple formulations of your prompt to either:\n","\n","1. Provide multiple options to users; or\n","2. Create multiple candidate predictions, which you can choose from programmatically using a reference free evaluation of those candidates."],"metadata":{"id":"2AQHZv0wa_yr"}},{"cell_type":"code","source":["template1 = \"\"\"Read the context below and answer the question. If the question cannot be answered based on the context alone or the context does not explicitly say the answer to the question, write \"Sorry I had trouble answering this question, based on the information I found.\"\n","\n","Context: {context}\n","\n","Question: {question}\n","\n","Answer: \"\"\"\n"," \n","prompt = PromptTemplate(\n","    input_variables=[\"context\", \"question\"],\n","    template=template,\n",")"],"metadata":{"id":"aw6S50morMHi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["template2 = \"\"\"Answer the question below based on the given context. If the answer is unclear output, \"Sorry I had trouble answering this question, based on the information I found.\"\n","\n","Context: {context}\n","Question: {question}\n","Answer: \"\"\"\n"," \n","prompt = PromptTemplate(\n","    input_variables=[\"context\", \"question\"],\n","    template=template,\n",")"],"metadata":{"id":"6OHvYDgGrRF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["context = \"Domino's gift cards are great for any person and any occasion. There are a number of different options to choose from. Each comes with a personalized card carrier and is delivered via US Mail.\"\n","question = \"How are gift cards delivered?\"\n","\n","answer1 = client.predict(name=\"default-text-gen\", data={\n","    \"prompt\": template1.format(context=context, question=question),\n","    \"temperature\": 0.1\n","})\n","\n","answer2 = client.predict(name=\"default-text-gen\", data={\n","    \"prompt\": template2.format(context=context, question=question),\n","    \"temperature\": 0.1\n","})\n","\n","answer3 = client.predict(name=\"default-text-gen\", data={\n","    \"prompt\": template1.format(context=context, question=question),\n","    \"temperature\": 0.8\n","})\n","\n","answer4 = client.predict(name=\"default-text-gen\", data={\n","    \"prompt\": template2.format(context=context, question=question),\n","    \"temperature\": 0.8\n","})\n","\n","for i, a in enumerate([answer1, answer2, answer3, answer4]):\n","  print(\"Answer\", str(i+1) + \": \", a)"],"metadata":{"id":"n-SwNEcYTeBl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683706120107,"user_tz":240,"elapsed":12347,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"7961407c-e480-43de-f607-c1609dab062e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Answer 1:  {'text': ' Gift cards are delivered via US Mail.'}\n","Answer 2:  {'text': ' Gift cards are delivered via US Mail with a personalized card carrier.'}\n","Answer 3:  {'text': ' Via US Mail.'}\n","Answer 4:  {'text': ' Gift cards are delivered via US Mail with a personalized card carrier.'}\n"]}]},{"cell_type":"code","source":["for i, a in enumerate([answer1, answer2, answer3, answer4]):\n","  factuality = client.predict(name=\"default-fact\", data={\n","      \"text\": a['text'],\n","      \"reference\": context\n","  })\n","  print(\"Answer\", str(i+1) + \" Factuality: \", \n","        str(int(factuality['probability']*100.0)) + \"% factual\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OR5mCSFRsfBB","executionInfo":{"status":"ok","timestamp":1683706284925,"user_tz":240,"elapsed":35341,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"03df8c23-c76e-4d29-8ea4-9b5fd6e45d7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Answer 1 Factuality:  98% factual\n","Answer 2 Factuality:  92% factual\n","Answer 3 Factuality:  98% factual\n","Answer 4 Factuality:  92% factual\n"]}]},{"cell_type":"markdown","source":["# Type checking, task formatting"],"metadata":{"id":"LJiOngWzTES0"}},{"cell_type":"markdown","source":["Reliability and consistency in LLM output is a major problem for the \"last mile\" of LLM integrations. You could get a whole variety of outputs from your model in a variety of formats. An increasing number of tools, including [Prediction Guard](https://www.predictionguard.com/), allow you to force a certain task structure or output type checking on your inferences. Another example of such a tool is the [Language Model Query Language](https://lmql.ai/)."],"metadata":{"id":"1minS7whbW57"}},{"cell_type":"code","source":["client.predict(name=\"default-text-gen\", data={\n","    \"prompt\": \"\"\"Assign a sentiment label to the text included below. Use the label NEU for neutral sentiment, NEG for negative sentiment, and POS for positive sentiment.\n","\n","Text: This workshop is spectacular. I love it! So wonderful.\n","Sentiment:\"\"\"\n","})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"txi_b4aJcCwb","executionInfo":{"status":"ok","timestamp":1683718428601,"user_tz":240,"elapsed":1238,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"a0c431e7-7de0-440f-e19c-9fa24e519d14"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'text': ' POS'}"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["client.predict(name=\"default-fact\", data={\n","    \"text\": \"The sky is green\",\n","    \"reference\": \"The sky is blue\"\n","})"],"metadata":{"id":"Uw8uy3VgTIXx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683706429000,"user_tz":240,"elapsed":2021,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"87e902f7-cbf9-4863-ced6-6f553b74933e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'factual': False, 'probability': 0.039652372413592346}"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["client.predict(name=\"default-sentiment\", data={\n","    \"phrase\": \"Everything is wonderful at this conference. What a great, amazing place!\"\n","})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u0YP7YvEuSvZ","executionInfo":{"status":"ok","timestamp":1683706835440,"user_tz":240,"elapsed":3385,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"287d4e24-5f73-44ba-afae-71b67a4c2522"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'sentiment': 'POS'}"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["client.predict(name=\"default-mt\", data={\n","    \"source\": \"eng\",\n","    \"target\": \"hin\",\n","    \"text\": \"Everything is wonderful at this conference. What a great, amazing place!\"\n","})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qaY9T5Yuv3Cx","executionInfo":{"status":"ok","timestamp":1683706876065,"user_tz":240,"elapsed":9945,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"ed4c1478-98f8-407a-d643-6d278115be03"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'translation': 'इस सम्मेलन में सब कुछ अद्भुत है। क्या एक महान, अद्भुत जगह है!'}"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["# Model selection\n","\n"],"metadata":{"id":"kZMoXZ1hSydJ"}},{"cell_type":"markdown","source":["Not all LLMs or pre-trained NLP models are going to perform the same on your data and prompts. There is a huge variability in fact! Don't just assume a single API, like OpenAI, is the model to rule them all. New models will come out making your integration irrelevant, or you might just have reliability issues from lock-in to a single provider/model. \n","\n","Prediction Guard can automatically evaluates hundreds of models (from OpenAI, Cohere, Hugging Face, etc.) and configure the best one for your use case, domain, or task. You could do this manually, but let's just do it with one API call and move on to other things."],"metadata":{"id":"Jvqps8ydcHMp"}},{"cell_type":"code","source":["examples = [\n","    {\n","        \"input\": {\n","            \"source\": \"hin\",\n","            \"target\": \"eng\",\n","            \"text\": \"Good, kya tumne actor jim carrey ke bare me suna hai\"\n","        },\n","        \"output\": {\n","            \"translation\": \"Good. Have you heard of actor jim carrey\"\n","        }\n","    },\n","    {\n","        \"input\": {\n","            \"source\": \"hin\",\n","            \"target\": \"eng\",\n","            \"text\": \"Bruce almight uski one of the best movie hai, kya tumne suna hai uske bare me\"\n","        },\n","        \"output\": {\n","            \"translation\": \"One of his best movies is Bruce almighty, have you heard of him\"\n","        }\n","    },\n","    {\n","        \"input\": {\n","            \"source\": \"hin\",\n","            \"target\": \"eng\",\n","            \"text\": \"maine suna hai, yah kiske bare me hai?\"\n","        },\n","        \"output\": {\n","            \"translation\": \"I've heard of it. Whats it about?\"\n","        }\n","    },\n","    {\n","        \"input\": {\n","            \"source\": \"hin\",\n","            \"target\": \"eng\",\n","            \"text\": \"Jim Carry ke liye kuch bhi sahi nahi ho raha hota hai aur god ko complaint karta hai, Morgan freeman ne god ka part kiya hai wo use ek week ke liye god bana deta hai\"\n","        },\n","        \"output\": {\n","            \"translation\": \"Jim Carrey things nothing goes right for him and complains to god. Morgan freeman plays god and lets him play god for a week\"\n","        }\n","    }\n","]"],"metadata":{"id":"s7P2uR9rQ_9Z","executionInfo":{"status":"ok","timestamp":1683718630344,"user_tz":240,"elapsed":234,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["client.create_proxy(name=\"hinglish-mt\", task=\"mt\", examples=examples)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jleKpw6Rw8pL","executionInfo":{"status":"ok","timestamp":1683709239474,"user_tz":240,"elapsed":44742,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"e610b2e9-d67d-456b-d9b7-81dd40988442"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating the proxy endpoint. Evaluating a bunch of SOTA models! -\n","\n","Proxy created successfully!\n","---------------------------\n","Name: hinglish-mt\n","Task: mt\n","Status: available\n","Failure Rate: 0.75\n"]}]},{"cell_type":"code","source":["client.predict(name=\"hinglish-mt\", data={\n","    \"source\": \"hin\",\n","    \"target\": \"eng\",\n","    \"text\": \"morgan tho yek acha aadmi hein. rotten tomatoes ne 48% deya jo isse bhi ache de sakthe they\"\n","})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eag-YtYvxSSF","executionInfo":{"status":"ok","timestamp":1683709281810,"user_tz":240,"elapsed":28624,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"5a51a296-340f-4dbb-832d-86918d8991e8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'translation': 'Morgan is a good man. Rotten Tomatoes gave 48% which could be even better.'}"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["client.delete_proxy(\"hinglish-mt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZzhT3wout7g","executionInfo":{"status":"ok","timestamp":1683709361677,"user_tz":240,"elapsed":916,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"44b5e42b-0fd3-4b21-a5d3-78e7662d4537"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Proxy deleted successfully!\n"]}]},{"cell_type":"code","source":["examples = [\n","    {\n","        \"input\": {\n","            \"source\": \"deu\",\n","            \"target\": \"eng\",\n","            \"text\": \"Am Anfang schuf Gott Himmel und Erde.\"\n","        },\n","        \"output\": {\n","            \"translation\": \"In the beginning God created the heavens and the earth.\"\n","        }\n","    },\n","    {\n","        \"input\": {\n","            \"source\": \"deu\",\n","            \"target\": \"eng\",\n","            \"text\": \"Noch war die Erde leer und ungestaltet, von tiefen Fluten bedeckt. Finsternis herrschte, aber über dem Wasser schwebte der Geist Gottes.\"\n","        },\n","        \"output\": {\n","            \"translation\": \"Now the earth was formless and empty, darkness was over the surface of the deep, and the Spirit of God was hovering over the waters.\"\n","        }\n","    },\n","    {\n","        \"input\": {\n","            \"source\": \"deu\",\n","            \"target\": \"eng\",\n","            \"text\": \"Da sprach Gott: »Licht soll entstehen!«, und sogleich strahlte Licht auf.\"\n","        },\n","        \"output\": {\n","            \"translation\": \"And God said, “Let there be light,” and there was light.\"\n","        }\n","    },\n","    {\n","        \"input\": {\n","            \"source\": \"deu\",\n","            \"target\": \"eng\",\n","            \"text\": \"Gott sah, dass es gut war. Er trennte das Licht von der Dunkelheit\"\n","        },\n","        \"output\": {\n","            \"translation\": \"God saw that the light was good, and he separated the light from the darkness.\"\n","        }\n","    }\n","]"],"metadata":{"id":"_JM90Imu14hn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["client.create_proxy(name=\"german-mt\", task=\"mt\", examples=examples)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EDWzfWl94g0i","executionInfo":{"status":"ok","timestamp":1683709918454,"user_tz":240,"elapsed":295487,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"c2065bce-67fb-4888-ac53-a7e9870907b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating the proxy endpoint. Evaluating a bunch of SOTA models! \\\n","\n","Proxy created successfully!\n","---------------------------\n","Name: german-mt\n","Task: mt\n","Status: available\n","Failure Rate: 0.5\n"]}]},{"cell_type":"code","source":["client.predict(name=\"german-mt\", data={\n","    \"source\": \"deu\",\n","    \"target\": \"eng\",\n","    \"text\": \"und nannte das Licht »Tag« und die Dunkelheit »Nacht«. Es wurde Abend und wieder Morgen: Der erste Tag war vergangen.\"\n","})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VTqo6Ooe4YiG","executionInfo":{"status":"ok","timestamp":1683709971281,"user_tz":240,"elapsed":26322,"user":{"displayName":"Daniel Whitenack","userId":"15195746376658990804"}},"outputId":"1c86ed71-46b3-478f-8614-5d4b8f730bfa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'translation': 'and called the light \"day\" and the darkness \"night.\" It was evening and again morning: the first day had passed.'}"]},"metadata":{},"execution_count":29}]}]}