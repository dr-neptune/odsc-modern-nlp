{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO9fScg9tEf0jCFxPMVnWEB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["We've actually already seen how it can be useful to \"chain\" various LLM operations together. In the Hinglish chat example we chained a response generation and then a machine translation using LLMs.\n","\n","**As you solve problems with LLMs, do NOT always think about your task as a single prompt.** Decompose your problem into multiple steps. Just like programming which uses multiple functions, classes, etc. LLM integration is a new kind of reasoning engine that you can \"program\" in a multi-step, conditional, control flow sort of fashion.\n","\n","Further, enterprise LLM appllications need reliability, trust, and consistency. **Because LLMs only predict probable text, they have no understanding or connection to reality.** This produces **hallucinations** that can be part of a coherent text block but factually (or otherwise) wrong. To deal with this we need to **ground** on LLM operations with external data. "],"metadata":{"id":"xm6EIVbldZer"}},{"cell_type":"markdown","source":["# Dependencies and imports"],"metadata":{"id":"wK6zNdTOdVcE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2yX1z4WyJP2"},"outputs":[],"source":["! pip install langchain predictionguard llama-index unstructured chromadb pdf2image pytesseract html2text"]},{"cell_type":"code","source":["! apt-get install -y poppler-utils tesseract-ocr libtesseract-dev"],"metadata":{"id":"kuiMDw9i9OEf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.document_loaders import PyPDFLoader\n","from langchain.vectorstores import Chroma\n","from langchain.document_loaders import UnstructuredPDFLoader\n","from langchain.llms import PredictionGuard\n","from langchain.chains.question_answering import load_qa_chain\n","from llama_index import (\n","    LLMPredictor,\n","    ServiceContext,\n","    GPTListIndex, \n","    GPTVectorStoreIndex,\n","    SimpleWebPageReader,\n","    StorageContext\n",")\n","import chromadb\n","from llama_index.embeddings.langchain import LangchainEmbedding\n","from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n","from llama_index.vector_stores import ChromaVectorStore\n","import predictionguard as pg\n","from langchain import PromptTemplate, FewShotPromptTemplate, LLMChain\n","import numpy as np"],"metadata":{"id":"Go5vRQcTycUl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["client = pg.Client(token=\"<your access token>\")"],"metadata":{"id":"HdSloPn7JTu0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# External knowledge in prompts"],"metadata":{"id":"EMW-amKXI-nk"}},{"cell_type":"markdown","source":["We've actually already seen external knowledge within our prompts. In the question and answer example, the `context` that we pasted in was a copy of phrasing on the Domino's website. "],"metadata":{"id":"UBxGhPzLebrA"}},{"cell_type":"code","source":["template = \"\"\"Read the context below and answer the question. If the question cannot be answered based on the context alone or the context does not explicitly say the answer to the question, write \"Sorry I had trouble answering this question, based on the information I found.\"\n","\n","Context: {context}\n","\n","Question: {question}\n","\n","Answer: \"\"\"\n"," \n","prompt = PromptTemplate(\n","    input_variables=[\"context\", \"question\"],\n","    template=template,\n",")"],"metadata":{"id":"vkmyGTHuJEc2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["context = \"Domino's gift cards are great for any person and any occasion. There are a number of different options to choose from. Each comes with a personalized card carrier and is delivered via US Mail.\"\n","\n","question = \"How are gift cards delivered?\"\n","\n","myprompt = prompt.format(context=context, question=question)\n","print(myprompt)"],"metadata":{"id":"XRkwJGTOJIAC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["client.predict(name=\"default-text-gen\", data={\n","    \"prompt\": myprompt,\n","    \"temperature\": 0.1\n","})"],"metadata":{"id":"sbyyGvyjJYCN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Chaining"],"metadata":{"id":"KMWMoiIRJvPW"}},{"cell_type":"markdown","source":["In order to make the insertion of such external knowledge (and the sequencing of LLM operations) easier, we are going to use a package called [LangChain](https://python.langchain.com/en/latest/index.html). LangChain allows us to create chains of operations like chaining a prompt template and an LLM prediction together. There are also pre-configured chains that add a bunch of convenience to our workflows!"],"metadata":{"id":"qOHnU0iMemvD"}},{"cell_type":"code","source":["llm_chain = LLMChain(prompt=prompt, \n","                     llm=PredictionGuard(token=\"n4HehSxYpKxQyhKX58IzqPjXa2pOOJ\"), \n","                     verbose=True)\n"," \n","question = \"How are gift cards delivered?\"\n","llm_chain.predict(question=question, context=context)"],"metadata":{"id":"En7HKGJNKDxU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Chaining with augmentation from documents"],"metadata":{"id":"rEYisXxsynLc"}},{"cell_type":"markdown","source":["You might have seen one example of augmentation/ retrieval from external data with the popular [ChatPDF](https://www.chatpdf.com/). With LangChain chains and our LLM, this type of \"answer questions out of your document\" can be implented quite quickly.\n","\n","To do this, we will:\n","\n","1. Load in a PDF\n","2. Load the pages of the PDF into a vector database (Chroma)\n","3. Use a QA chain from LangChain to execute retrieval based question answering over the document.\n","\n"],"metadata":{"id":"j1uxTaKNe-VD"}},{"cell_type":"code","source":["! wget https://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf"],"metadata":{"id":"emiPbAQk7wXF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use the convenience of LangChain to load the PDF into pages and create\n","# a vector database from the document.\n","loader = UnstructuredPDFLoader(\"rules_of_ml.pdf\")\n","pages = loader.load_and_split()\n","docsearch = Chroma.from_documents(pages).as_retriever()"],"metadata":{"id":"2YSutKOOyrCA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ask a question of the document.\n","query = \"What does it mean to launch and iterate?\"\n","docs = docsearch.get_relevant_documents(query)\n","chain = load_qa_chain(PredictionGuard(token=\"<your access token>\",\n","                                      name=\"default-text-gen\"), chain_type=\"stuff\")\n","output = chain.run(input_documents=docs, question=query)\n","print(output)"],"metadata":{"id":"8ESqquvH67TQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Augmentation with information on the Internet"],"metadata":{"id":"k9W63fC3-xtH"}},{"cell_type":"markdown","source":["This time we will use some slightly different tools including [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/). LlamaIndex is a project that provides a central interface to connect your LLMâ€™s with external data. There are all sorts of really powerful indices, data stores, and query structures to retrieve and generate output from websites, documents, databases, Slack, etc.\n","\n","We will use a website connector to parse a website, convert it to text, and then query information out of the website. "],"metadata":{"id":"a3T8ifycf0zL"}},{"cell_type":"code","source":["# Define an embedding model (which will be used with our vector database)\n","model_name = \"sentence-transformers/all-mpnet-base-v2\"\n","emb = LangchainEmbedding(HuggingFaceEmbeddings(model_name=model_name))  "],"metadata":{"id":"FUMGDJjbGQdP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the LLM for LlamaIndex\n","llm_predictor = LLMPredictor(llm=PredictionGuard(token=\"<your access token>\",\n","                                      name=\"default-text-gen\"))\n","service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor,\n","                                               embed_model=emb)"],"metadata":{"id":"H21Uv7UQ-1PZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read a page from Paul Graham's website.\n","documents = SimpleWebPageReader(html_to_text=True).load_data([\"http://paulgraham.com/worked.html\"])"],"metadata":{"id":"nouZnS56Anmn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup our vector database\n","chroma_client = chromadb.Client()\n","chroma_collection = chroma_client.create_collection(\"paul_graham\")\n","vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n","storage_context = StorageContext.from_defaults(vector_store=vector_store)"],"metadata":{"id":"huhhN9BJEYv_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the \"Index\" that will be used as the backbone of our queries. This is\n","# composed of \"nodes\" that can be queried using various techniques (vector\n","# based search, LLM summary search, etc.)\n","index = GPTVectorStoreIndex.from_documents(\n","    documents, \n","    storage_context=storage_context,\n","    service_context=service_context)"],"metadata":{"id":"ThOKuqaqAv7M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Query the website!\n","query_engine = index.as_query_engine()\n","response = query_engine.query(\"What did the author do growing up?\")\n","print(response.response)"],"metadata":{"id":"Jwr2VoFDBD06"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"61seNiMkIlYN"},"execution_count":null,"outputs":[]}]}